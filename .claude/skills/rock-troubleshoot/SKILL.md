---
name: rock-troubleshoot
description: Review and troubleshoot ROCK R3Pro lidar acquisition data and ROCK Desktop processing issues. Use when investigating missing point clouds, timestamp errors, data corruption, trajectory problems, or processing failures with ROCK R3Pro datasets.
argument-hint: [acquisition-dir or file-path]
user-invocable: true
allowed-tools: Read, Grep, Bash(python *), Bash(xxd *), Bash(ls *), Bash(head *), Bash(tail *), Bash(wc *), Bash(cat *), Bash(unzip *)
model: opus
---

# ROCK R3Pro Data Review & Troubleshooting

You are troubleshooting ROCK R3Pro lidar acquisition data and ROCK Desktop processing issues.

If a path argument is provided: `$ARGUMENTS`

## Acquisition Directory Structure

A ROCK R3Pro acquisition directory follows this structure:
```
20260205_141743_1/                    # Named: YYYYMMDD_HHMMSS_N
├── acquisition-config.json           # Processing step 1 output (timestamps, paths, sensor info)
├── device_info.json                  # Device calibration, lever arms, lidar/camera config
├── Cam1/
│   ├── Airborne_CameraParams.json    # Camera intrinsic calibration
│   ├── *_1.cpt                       # Camera capture timestamps (CSV)
│   ├── *_1.tmp                       # Camera temporary timestamps (CSV)
│   └── *.JPG                         # Camera images (numbered SEAG######)
├── Lidar1/
│   └── *_1.mlr                       # Raw lidar data (Hesai XT32 packets, typically 5-15GB)
├── POS/
│   ├── *-gnss.epp                    # GPS ephemeris (ASCII)
│   ├── *-gnss.gpb                    # GPS raw observations (binary)
│   ├── *-gnss.gps                    # GPS solution (binary)
│   ├── *-imu.imr                     # IMU raw data (binary)
│   ├── *-imu.ins                     # INS solution (binary)
│   └── base/                         # Base station data (may be empty)
└── processed/
    ├── processing_report.html        # Visual report with charts
    ├── ROCK_imagelist.csv            # Georeferenced image list
    ├── ROCK_ppk.txt                  # PPK solution text
    └── .tmp/
        ├── copc_stats.json           # Point cloud statistics
        ├── full-trajectory.laz       # Trajectory as LAZ
        ├── ranges.json               # Lidar GPS time ranges
        ├── results.json              # Full trajectory results
        ├── reprojected.json          # Reprojected trajectory
        ├── rock.fml / rock.rml       # Forward/reverse trajectory logs
        ├── rock.fss / rock.rss       # Forward/reverse solution summaries
        ├── rock.ft / rock.rt         # Forward/reverse trajectories
        ├── rock.proj                 # NovAtel Waypoint project file
        ├── SBET.OUT                  # Smoothed best estimate trajectory
        ├── converted/                # Point cloud output (LAZ files go here)
        └── factory/
            └── *.json                # Factory sensor calibration
```

## Diagnostic Workflow

### Step 1: Identify the acquisition directory
If not provided, ask the user. Then list contents:
```bash
ls -laR "<acquisition-dir>/"
```

### Step 2: Read configuration files
Read both JSON configs to understand the dataset:

**acquisition-config.json** contains:
- `start_time_utc` / `end_time_utc` - Unix timestamps (with sub-second precision)
- `sensor` - Should be "R3PRO"
- `serial_number` - Device serial
- `firmware_version` - Firmware version string
- Paths to GPS and IMU files

**device_info.json** contains:
- `Lidar_Start_Time` / `Lidar_End_Time` - Format: "YYYYMMDDHHMMSS" (local time)
- `Time_Zone` - Offset from UTC (e.g., -8.0 for PST)
- `Leap_Second` - GPS leap seconds (typically -18)
- `Point_Frequency` - Points per second (e.g., 1280000 for 1.28 MHz)
- `Scan_Frequency` - Rotations per second (e.g., 10)
- Lever arm offsets (IMU_to_GNSS, Lidar_in_Body, Cam_in_Body)

### Step 3: Validate timestamps
Cross-reference timestamps between the two config files:

```python
import datetime

# Convert Lidar_Start_Time from device_info.json
# Format is YYYYMMDDHHMMSS in local time
# Convert to UTC using Time_Zone offset, then to Unix timestamp

# Verify start_time_utc and end_time_utc in acquisition-config.json
# are consistent with Lidar_Start_Time/Lidar_End_Time

# KNOWN BUG: end_time_utc may be wildly incorrect (off by ~100 days)
# Always cross-check against Lidar_End_Time
```

**Timestamp cross-validation formula:**
```
local_time = parse("YYYYMMDDHHMMSS" from Lidar_Start_Time)
utc_time = local_time - Time_Zone hours
unix_timestamp = utc_time as Unix epoch seconds

# Compare with start_time_utc / end_time_utc
# They should match within a few seconds
```

**Known issue:** The `end_time_utc` field in `acquisition-config.json` is generated by ROCK Desktop processing step 1 and has been observed to be completely wrong (off by ~104 days / ~9 million seconds). The `Lidar_End_Time` string in `device_info.json` is authoritative. If correcting, calculate from start_time_utc + duration:
```
corrected_end = start_time_utc + (Lidar_End_Time_UTC - Lidar_Start_Time_UTC)
```
This preserves the sub-second decimal precision of the original start timestamp.

### Step 4: Validate camera timestamps
The `.cpt` file contains camera capture times:
```
filename,year,month,day,hour,minute,second,???@nanosecond_counter
```
- Timestamps are in **UTC+8** (device internal clock), NOT local time or UTC
- The nanosecond counter is relative to acquisition start (~1 GHz clock)
- Verify first/last image times match expected acquisition window
- Delta between consecutive images should match Photo Interval (typically 2s)

### Step 5: Check MLR (lidar) file integrity
The MLR file wraps Hesai XT32 lidar packets:

**Header structure (offset 0x0000):**
```
Bytes 0-2:   "MLR\0" magic
Bytes 4-5:   Year (little-endian 16-bit, e.g., 0x07EA = 2026)
Byte 6:      Month
Byte 7:      Day
Byte 8:      Hour (local time)
Byte 9:      Minute
Byte 10:     Second
Offset 0x110: Sensor ID string (e.g., "XT32")
```

**Data integrity check - scan packet marker density:**
```python
import os
path = r'<mlr_file_path>'
size = os.path.getsize(path)
with open(path, 'rb') as f:
    chunk_size = 100 * 1024 * 1024  # 100MB chunks
    offset = 0
    while offset < size:
        f.seek(offset)
        chunk = f.read(chunk_size)
        count = chunk.count(b'\xee\xff')
        pct = offset * 100 / size
        print(f'Offset {offset/1e9:.1f}GB ({pct:.0f}%): {count} eeff markers')
        offset += chunk_size
```

**Expected pattern:**
- First ~200MB: Very high density (~97,000 markers per 100MB) - this is the packet index/header region
- Remaining file: Lower but consistent density (~25-45 per 100MB) - raw lidar data in denser binary format
- A sudden drop to 0 markers or large blocks of null bytes indicates **corruption**

**File size validation:**
```
Expected size ~= duration_seconds * point_frequency * ~4 bytes/point
Example: 1800s * 1,280,000 pts/s * ~4.2 bytes = ~9.7 GB
```

**ZIP transfer corruption check:**
- Compare file size (in bytes) between source and destination - must match exactly
- Files >4GB require ZIP64 support; Windows built-in zip handles this correctly
- Non-Windows extraction tools (7zip, unzip on Linux/Mac) may handle ZIP64 differently
- If sizes match, file is almost certainly intact

### Step 6: Check processing output

**Trajectory quality (rock.fss / rock.rss):**
- Look for `Status: "SUCCESS"` at the end
- Check ARTK engagement: should show valid integer fix
- `NumEpochs` should be reasonable for the duration

**Trajectory log (rock.fml / rock.rml):**
- Look for `"Failure to invert normal"` errors (occasional is OK, frequent is bad)
- `"ARTK not currently engaged--ARTK fix ignored"` during forward/reverse pass is normal
- Check that ARTK obtained valid fix at startup

**Processing report (processing_report.html):**
- GNSS/IMU overlap chart: Remote and IMU bars should be fully contained within Master bar
- If Master (base station) doesn't cover the acquisition window, need a different base file
- Tightly Coupled Results: Roll/Pitch accuracy should be <0.01 deg, Heading <0.1 deg
- Horizontal position accuracy should be <0.01m for good results

**Point cloud output:**
- Check `processed/.tmp/converted/` directory - this is where LAZ files should appear
- If empty, point cloud generation failed (trajectory may still be OK)
- Check `copc_stats.json` for point count and density
- Check `ranges.json` for lidar GPS time ranges - these are in **GPS time** (not Unix time)

**GPS time vs Unix time:**
```
GPS epoch = January 6, 1980 00:00:00 UTC
Unix epoch = January 1, 1970 00:00:00 UTC
Offset = 315,964,800 seconds
Leap seconds = 18 (as of 2026)

unix_time = gps_time + 315964800 - leap_seconds
gps_time = unix_time - 315964800 + leap_seconds
```

### Step 7: GPS time of week reference
Processing logs use GPS Time of Week (seconds since Sunday 00:00:00 UTC):
```
GPS_TOW = (day_of_week * 86400) + (hour * 3600) + (minute * 60) + second
# Monday = 1, Sunday = 0
# Max value: 604800 (one week)
```

## Common Issues & Solutions

### "No points" / Empty converted directory
**Symptoms:** Trajectory processes OK, but no point cloud output
**Possible causes:**
1. Lidar-to-trajectory time synchronization mismatch
2. MLR file corruption from transfer
3. ROCK Desktop software bug
4. `end_time_utc` grossly incorrect (known bug)

**Diagnostic steps:**
1. Verify `converted/` directory is empty
2. Check `ranges.json` exists and has valid GPS time ranges
3. Verify MLR file integrity (marker density scan)
4. Compare MLR file size with original source
5. Check `end_time_utc` against `Lidar_End_Time`

### Incorrect end_time_utc
**Symptoms:** `end_time_utc` in acquisition-config.json is wildly different from expected
**Root cause:** Bug in ROCK Desktop processing step 1
**Fix:** Calculate correct value from `start_time_utc` + acquisition duration:
```python
# Duration from Lidar times in device_info.json
start_local = parse(Lidar_Start_Time)  # YYYYMMDDHHMMSS
end_local = parse(Lidar_End_Time)
duration = (end_local - start_local).total_seconds()
corrected_end = start_time_utc + duration
# This preserves original decimal precision
```

### Base station coverage insufficient
**Symptoms:** Processing report shows Master bar doesn't fully contain Remote/IMU bars
**Fix:** Obtain a base station file that covers the full acquisition window plus margins

### Poor trajectory accuracy
**Symptoms:** High attitude/position RMS values in processing report
**Possible causes:**
1. Insufficient satellite coverage
2. Bad base station data
3. Missing calibration maneuvers (need high-speed runs at start/end)
4. Large baseline distance to base station

### Data transfer corruption
**Symptoms:** Processing fails, file size mismatch, or unexpected null blocks in MLR
**Prevention:**
- Always verify file sizes match after transfer
- Use checksum verification (MD5/SHA256) for large files
- Prefer robocopy or rsync over drag-and-drop for large datasets
- Windows built-in zip supports ZIP64 for files >4GB

## ROCK Desktop Software Info
- Processing software: ROCK Desktop (e.g., version 1.16.5)
- Uses NovAtel Waypoint (Inertial Explorer) engine for trajectory processing
- PPK solution uses ARTK (Ambiguity Resolution Tight Kinematic)
- Supports GPS, GLONASS, BeiDou, Galileo constellations
- Outputs LAZ (compressed LAS) point clouds
